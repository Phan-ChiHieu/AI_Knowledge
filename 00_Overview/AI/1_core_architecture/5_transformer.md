# Transformer lÃ  má»™t kiáº¿n trÃºc deep learning dÃ¹ng Ä‘á»ƒ xá»­ lÃ½ chuá»—i dá»¯ liá»‡u

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” âˆ˜â—¦ âœ§ âœ¦ âœ§ â—¦âˆ˜ â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

## ğŸš€ WHAT â€“ BÃªn trong Transformer lÃ  gÃ¬?

Transformer lÃ  má»™t kiáº¿n trÃºc **deep learning** dÃ¹ng Ä‘á»ƒ **xá»­ lÃ½ chuá»—i dá»¯ liá»‡u** (vÃ­ dá»¥: vÄƒn báº£n). NÃ³ Ä‘Æ°á»£c giá»›i thiá»‡u trong paper ná»•i tiáº¿ng _"Attention is All You Need" (2017)._

Cáº¥u trÃºc Transformer gá»“m 2 khá»‘i chÃ­nh:

1. `Encoder` (mÃ£ hoÃ¡) â†’ dÃ¹ng trong BERT, dá»‹ch mÃ¡y.

2. `Decoder` (giáº£i mÃ£) â†’ dÃ¹ng trong GPT, sinh vÄƒn báº£n.

> GPT chá»‰ dÃ¹ng Decoder stack (Uni-directional).<br />
> BERT chá»‰ dÃ¹ng Encoder stack (Bi-directional).<br /> > _TÃ³m láº¡i: LLM = nhiá»u lá»›p Transformer Decoder (trong GPT)_

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” âˆ˜â—¦ âœ§ âœ¦ âœ§ â—¦âˆ˜ â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

## ğŸ§  WHY â€“ VÃ¬ sao Transformer máº¡nh?

TrÆ°á»›c Transformer, ngÆ°á»i ta dÃ¹ng cÃ¡c kiáº¿n trÃºc: RNN, LSTM Ä‘á»ƒ xá»­ lÃ½ chuá»—i â†’ nhÆ°ng cÃ¡c mÃ´ hÃ¬nh nÃ y **khÃ³ há»c ngá»¯ cáº£nh xa vÃ  khÃ´ng song song hoÃ¡ Ä‘Æ°á»£c**.

Transformer dÃ¹ng `Attention` (Táº­p trung) Ä‘á»ƒ:

- NhÃ¬n toÃ n bá»™ chuá»—i 1 lÃºc (khÃ´ng tuáº§n tá»± nhÆ° RNN)
- Tá»± chá»n pháº§n nÃ o cá»§a chuá»—i Ä‘áº§u vÃ o lÃ  "quan trá»ng"
- Song song hÃ³a cá»±c nhanh trÃªn GPU â†’ huáº¥n luyá»‡n mÃ´ hÃ¬nh cá»±c lá»›n

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” âˆ˜â—¦ âœ§ âœ¦ âœ§ â—¦âˆ˜ â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

## âš™ï¸ HOW â€“ Transformer hoáº¡t Ä‘á»™ng tháº¿ nÃ o? (Chi tiáº¿t dá»… hiá»ƒu)

### Tá»•ng thá»ƒ kiáº¿n trÃºc má»™t Decoder layer (cá»§a GPT):

```bash
Input tokens â†’ Embedding â†’ Positional Encoding
â†“
Layer 1:
  Self-Attention â†’ Add & Norm
  FeedForward â†’ Add & Norm
â†“
Layer 2:
  ...
â†“
Output logits â†’ chá»n tá»« tiáº¿p theo

```

### 1. **Input Token & Embedding**

- VÄƒn báº£n: `"The cat sat"` â†’ Token hÃ³a â†’ `[101, 232, 674]`
- Ãnh xáº¡ token thÃ nh vector (embedding) â†’ vÃ­ dá»¥ `101 â†’ [0.2, 0.5, -0.1, ...]`

### 2. **Positional Encoding**

- VÃ¬ Transformer khÃ´ng xá»­ lÃ½ tuáº§n tá»± nhÆ° RNN nÃªn cáº§n thÃªm vá»‹ trÃ­ token Ä‘á»ƒ hiá»ƒu ngá»¯ cáº£nh (dÃ¹ng sin/cos hoáº·c embedding vá»‹ trÃ­).

### 3. **Self-Attention (Cá»‘t lÃµi)**

#### Má»¥c tiÃªu:

> Má»—i tá»« cáº§n **"nhÃ¬n láº¡i" toÃ n bá»™ cÃ¢u** Ä‘á»ƒ quyáº¿t Ä‘á»‹nh pháº§n nÃ o quan trá»ng.

#### CÃ¡ch lÃ m:

- Vá»›i má»—i token, táº¡o ra 3 vector: **Query (Q), Key (K), Value (V)**
- TÃ­nh attention score:
  $\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) \cdot V$

â†’ Giá»‘ng nhÆ°:

> â€œTÃ´i lÃ  token `sat`, tÃ´i muá»‘n biáº¿t tá»« nÃ o quan trá»ng vá»›i tÃ´i â†’ dÃ¹ng Q, K Ä‘á»ƒ tÃ­nh má»©c Ä‘á»™ liÃªn quan â†’ sau Ä‘Ã³ cá»™ng cÃ¡c V theo trá»ng sá»‘.â€

### 4. **FeedForward Network**

- Má»—i token sau attention sáº½ qua 1 MLP nhá» (giá»‘ng fully-connected layer).

### 5. **Residual + LayerNorm**

- Má»—i bÆ°á»›c Ä‘á»u cÃ³:
  `Output = LayerNorm(x + SubLayer(x))`
  â†’ giÃºp á»•n Ä‘á»‹nh vÃ  gradient khÃ´ng bá»‹ máº¥t.

## ğŸ” Stack N láº§n cÃ¡c layer trÃªn

VÃ­ dá»¥ GPT-3 cÃ³ **96 layers**, GPT-4 cÃ²n nhiá»u hÆ¡n â†’ mÃ´ hÃ¬nh cÃ³ thá»ƒ há»c Ä‘Æ°á»£c má»‘i liÃªn há»‡ ngÃ´n ngá»¯ ráº¥t phá»©c táº¡p.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” âˆ˜â—¦ âœ§ âœ¦ âœ§ â—¦âˆ˜ â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

## ğŸ§ª VÃ­ dá»¥ minh há»a Attention

CÃ¢u:

> "The cat sat on the mat because it was tired."

â†’ Token `"it"` sáº½ **tá»± Ä‘á»™ng há»c Ä‘Æ°á»£c** ráº±ng "it" liÃªn quan Ä‘áº¿n "the cat", chá»© khÃ´ng pháº£i "the mat".
â†’ VÃ¬ attention sáº½ gÃ¡n trá»ng sá»‘ cao cho "cat" khi tÃ­nh tá»« `"it"`.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” âˆ˜â—¦ âœ§ âœ¦ âœ§ â—¦âˆ˜ â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

## Tá»•ng káº¿t theo nguyÃªn táº¯c 80/20:

| ThÃ nh pháº§n              | Ã nghÄ©a                                |
| ----------------------- | -------------------------------------- |
| **Embedding**           | Chuyá»ƒn token â†’ vector                  |
| **Positional Encoding** | ThÃªm thÃ´ng tin vá»‹ trÃ­                  |
| **Self-Attention**      | TÃ­nh má»‘i liÃªn há»‡ giá»¯a cÃ¡c token        |
| **FeedForward**         | Ãnh xáº¡ phi tuyáº¿n tá»«ng token            |
| **Stack Layers**        | Láº·p láº¡i nhiá»u láº§n Ä‘á»ƒ há»c biá»ƒu diá»…n sÃ¢u |
